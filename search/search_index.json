{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Lightweight CNNs for chess board analysis <p>Project Status</p> <p>This project is actively developed. The current version provides a complete pipeline for training lightweight CNNs to classify chess board elements from 32\u00d732px square images.</p> <p>Chess CV is a machine learning project that trains lightweight CNNs (156k parameters each) from scratch to classify different aspects of chess board squares. The project includes three specialized models trained on synthetically generated data from 55 board styles combined with piece sets and overlays from chess.com and lichess:</p> <ul> <li>Pieces Model (13 classes): Classifies chess pieces and empty squares for board state recognition and FEN generation</li> <li>Arrows Model (49 classes): Classifies arrow annotation components for detecting and reconstructing chess analysis overlays</li> <li>Snap Model (2 classes): Classifies piece centering quality for automated board analysis and positioning validation</li> </ul> <p>Each model uses the same efficient CNN architecture but is optimized for its specific classification task, achieving robust recognition across various visual styles.</p> <ul> <li> <p> Setup</p> <p>Installation guide covering dependencies and environment setup.</p> <p> Setup</p> </li> <li> <p> Model Usage</p> <p>Use pre-trained models from Hugging Face Hub or the chess-cv library in your projects.</p> <p> Model Usage</p> </li> <li> <p> Train and Evaluate</p> <p>Learn how to generate data, train models, and evaluate performance.</p> <p> Train and Evaluate</p> </li> <li> <p> Documentation for LLM</p> <p>Documentation in llms.txt format. Just paste the following link into the LLM chat.</p> <p> llms-full.txt</p> </li> </ul>"},{"location":"README_hf/","title":"README hf","text":"# Chess CV   <p>Lightweight CNNs (156k parameters each) for chess board analysis from 32\u00d732 pixel square images. The project includes three specialized models trained on synthetic data from chess.com/lichess boards, piece sets, arrow overlays, and centering variations:</p> <ul> <li>Pieces Model: Classifies 13 classes (6 white pieces, 6 black pieces, empty squares) for board state recognition and FEN generation</li> <li>Arrows Model: Classifies 49 classes representing arrow overlay patterns for detecting chess analysis annotations</li> <li>Snap Model: Classifies 2 classes (centered vs off-centered pieces) for automated board analysis and piece positioning validation</li> </ul>","tags":["computer-vision","image-classification","chess","cnn","lightweight"]},{"location":"README_hf/#quick-start","title":"Quick Start","text":"<pre><code>pip install chess-cv\n</code></pre> <pre><code>from chess_cv import load_bundled_model\n\n# Load pre-trained models (weights included in package)\npieces_model = load_bundled_model('pieces')\narrows_model = load_bundled_model('arrows')\nsnap_model = load_bundled_model('snap')\n\n# Make predictions\npiece_predictions = pieces_model(image_tensor)\narrow_predictions = arrows_model(image_tensor)\nsnap_predictions = snap_model(image_tensor)\n</code></pre> <p>Alternative: Load latest version from Hugging Face Hub</p> <pre><code>from huggingface_hub import hf_hub_download\nfrom chess_cv.model import SimpleCNN\nimport mlx.core as mx\n\n# Download latest weights from Hugging Face\nmodel_path = hf_hub_download(repo_id=\"S1M0N38/chess-cv\", filename=\"pieces.safetensors\")\nmodel = SimpleCNN(num_classes=13)\nweights = mx.load(str(model_path))\nmodel.load_weights(list(weights.items()))\nmodel.eval()\n</code></pre>","tags":["computer-vision","image-classification","chess","cnn","lightweight"]},{"location":"README_hf/#models","title":"Models","text":"<p>This repository contains three specialized models for chess board analysis:</p>","tags":["computer-vision","image-classification","chess","cnn","lightweight"]},{"location":"README_hf/#pieces-model-piecessafetensors","title":"\u265f\ufe0f Pieces Model (<code>pieces.safetensors</code>)","text":"<p>Overview:</p> <p>The pieces model classifies chess square images into 13 classes: 6 white pieces (wP, wN, wB, wR, wQ, wK), 6 black pieces (bP, bN, bB, bR, bQ, bK), and empty squares (xx). This model is designed for board state recognition and FEN generation from chess board images.</p> <p>Training:</p> <ul> <li>Architecture: SimpleCNN (156k parameters)</li> <li>Input: 32\u00d732px RGB square images</li> <li>Data: ~93,000 synthetic images from 55 board styles \u00d7 64 piece sets</li> <li>Augmentation: Aggressive augmentation with arrow overlays (80%), highlight overlays (25%), move overlays (50%), random crops, horizontal flips, color jitter, rotation (\u00b110\u00b0), and Gaussian noise</li> <li>Optimizer: AdamW (weight_decay=0.001) with LR scheduler (warmup + cosine decay: 0\u21920.001\u21921e-5)</li> <li>Training: 1000 epochs, batch size 64</li> </ul> <p>Performance:</p> Dataset Accuracy F1-Score (Macro) Test Data 99.90% 99.90% S1M0N38/chess-cv-openboard * - 98.56% S1M0N38/chess-cv-chessvision * - 92.28% <p>* Dataset with unbalanced class distribution (e.g. many more samples for empty square class), so accuracy is not representative.</p>","tags":["computer-vision","image-classification","chess","cnn","lightweight"]},{"location":"README_hf/#arrows-model-arrowssafetensors","title":"\u2197 Arrows Model (<code>arrows.safetensors</code>)","text":"<p>Overview:</p> <p>The arrows model classifies chess square images into 49 classes representing different arrow overlay patterns: 20 arrow heads, 12 arrow tails, 8 middle segments (for straight and diagonal arrows), 4 corner pieces (for knight-move arrows), and empty squares (xx). This model enables detection and reconstruction of arrow annotations commonly used in chess analysis interfaces. The NSEW naming convention (North/South/East/West) indicates arrow orientation and direction.</p> <p>Training:</p> <ul> <li>Architecture: SimpleCNN (156k parameters, same as pieces model)</li> <li>Input: 32\u00d732px RGB square images</li> <li>Data: ~4.5M synthetic images from 55 board styles \u00d7 arrow overlays (~3.14M train, ~672K val, ~672K test)</li> <li>Augmentation: Conservative augmentation with highlight overlays (25%), move overlays (50%), random crops, and minimal color jitter/noise. No horizontal flips to preserve arrow directionality</li> <li>Optimizer: AdamW (lr=0.0005, weight_decay=0.00005)</li> <li>Training: 20 epochs, batch size 128</li> </ul> <p>Performance:</p> Dataset Accuracy F1-Score (Macro) Test Data (synthetic) 99.99% 99.99% <p>The arrows model is optimized for detecting directional annotations while maintaining spatial consistency across the board.</p> <p>Limitation: Classification accuracy degrades when multiple arrow components overlap in a single square.</p>","tags":["computer-vision","image-classification","chess","cnn","lightweight"]},{"location":"README_hf/#snap-model-snapsafetensors","title":"\ud83d\udcd0 Snap Model (<code>snap.safetensors</code>)","text":"<p>Overview:</p> <p>The snap model classifies chess square images into 2 classes: centered (\"ok\") and off-centered (\"bad\") pieces. This model is designed for automated board analysis and piece positioning validation, helping ensure proper piece placement in digital chess interfaces and automated analysis systems.</p> <p>Training:</p> <ul> <li>Architecture: SimpleCNN (156k parameters)</li> <li>Input: 32\u00d732px RGB square images</li> <li>Data: ~1.4M synthetic images from centered and off-centered piece positions (~985,960 train, ~211,574 validate, ~210,466 test)</li> <li>Augmentation: Conservative augmentation with arrow overlays (50%), highlight overlays (20%), move overlays (50%), mouse overlays (80%), horizontal flips (50%), color jitter, and Gaussian noise. No rotation or geometric transformations to preserve centering semantics</li> <li>Optimizer: AdamW (weight_decay=0.001) with LR scheduler (warmup + cosine decay: 0\u21920.001\u21921e-5)</li> <li>Training: 200 epochs, batch size 64</li> </ul> <p>Performance:</p> Dataset Accuracy F1-Score (Macro) Test Data (synthetic) 99.93% 99.93% <p>The snap model is optimized for detecting piece centering issues while maintaining robustness to various board styles and visual conditions.</p> <p>Use Cases:</p> <ul> <li>Automated board state validation</li> <li>Piece positioning quality control</li> <li>Chess interface usability testing</li> <li>Digital chess board quality assurance</li> </ul>","tags":["computer-vision","image-classification","chess","cnn","lightweight"]},{"location":"README_hf/#training-your-own-model","title":"Training Your Own Model","text":"<p>To train or evaluate a model yourself:</p> <pre><code>git clone https://github.com/S1M0N38/chess-cv.git\ncd chess-cv\nuv sync --all-extras\n\n# Generate training data for a specific model\nchess-cv preprocessing pieces  # or 'arrows' or 'snap'\n\n# Train model\nchess-cv train pieces  # or 'arrows' or 'snap'\n\n# Evaluate model\nchess-cv test pieces  # or 'arrows' or 'snap'\n</code></pre> <p>See the Setup Guide and Train and Evaluate for detailed instructions on data generation, training configuration, and evaluation.</p>","tags":["computer-vision","image-classification","chess","cnn","lightweight"]},{"location":"README_hf/#limitations","title":"Limitations","text":"<ul> <li>Requires precisely cropped 32\u00d732 pixel square images (no board detection)</li> <li>Trained on synthetic data; may not generalize to real-world photos</li> <li>Not suitable for non-standard piece designs</li> <li>Optimized for Apple Silicon (slower on CPU)</li> </ul> <p>For detailed documentation, architecture details, and advanced usage, see the full documentation.</p>","tags":["computer-vision","image-classification","chess","cnn","lightweight"]},{"location":"README_hf/#citation","title":"Citation","text":"<pre><code>@software{bertolotto2025chesscv,\n  author = {Bertolotto, Simone},\n  title = {{Chess CV}},\n  url = {https://github.com/S1M0N38/chess-cv},\n  year = {2025}\n}\n</code></pre>   **Repo:** [github.com/S1M0N38/chess-cv](https://github.com/S1M0N38/chess-cv) \u2022 **PyPI:** [pypi.org/project/chess-cv](https://pypi.org/project/chess-cv/)","tags":["computer-vision","image-classification","chess","cnn","lightweight"]},{"location":"architecture/","title":"Architecture","text":"<p>Detailed information about the Chess CV model architectures, training strategies, and performance characteristics for the pieces, arrows, and snap models.</p> <p> </p> CNN architecture for chess piece classification"},{"location":"architecture/#pieces-model","title":"Pieces Model","text":""},{"location":"architecture/#model-architecture","title":"Model Architecture","text":"<p>Chess CV uses a lightweight Convolutional Neural Network (CNN) designed for efficient inference while maintaining high accuracy on 32\u00d732 pixel chess square images.</p>"},{"location":"architecture/#network-design","title":"Network Design","text":"<pre><code>Input: 32\u00d732\u00d73 RGB image\n\nConv Layer 1:\n\u251c\u2500\u2500 Conv2d(3 \u2192 16 channels, 3\u00d73 kernel)\n\u251c\u2500\u2500 ReLU activation\n\u2514\u2500\u2500 MaxPool2d(2\u00d72) \u2192 16\u00d716\u00d716\n\nConv Layer 2:\n\u251c\u2500\u2500 Conv2d(16 \u2192 32 channels, 3\u00d73 kernel)\n\u251c\u2500\u2500 ReLU activation\n\u2514\u2500\u2500 MaxPool2d(2\u00d72) \u2192 8\u00d78\u00d732\n\nConv Layer 3:\n\u251c\u2500\u2500 Conv2d(32 \u2192 64 channels, 3\u00d73 kernel)\n\u251c\u2500\u2500 ReLU activation\n\u2514\u2500\u2500 MaxPool2d(2\u00d72) \u2192 4\u00d74\u00d764\n\nFlatten \u2192 1024 features\n\nFully Connected 1:\n\u251c\u2500\u2500 Linear(1024 \u2192 128)\n\u251c\u2500\u2500 ReLU activation\n\u2514\u2500\u2500 Dropout(0.5)\n\nFully Connected 2:\n\u2514\u2500\u2500 Linear(128 \u2192 13) \u2192 Output logits\n\nSoftmax \u2192 13-class probabilities\n</code></pre>"},{"location":"architecture/#model-statistics","title":"Model Statistics","text":"<ul> <li>Total Parameters: 156,077</li> <li>Trainable Parameters: 156,077</li> <li>Model Size: ~600 KB (safetensors format)</li> <li>Input Size: 32\u00d732\u00d73 (RGB)</li> <li>Output Classes: 13</li> </ul>"},{"location":"architecture/#class-labels","title":"Class Labels","text":"<p>The model classifies chess squares into 13 categories:</p> <p>Black Pieces (6):</p> <ul> <li><code>bB</code> \u2013 Black Bishop</li> <li><code>bK</code> \u2013 Black King</li> <li><code>bN</code> \u2013 Black Knight</li> <li><code>bP</code> \u2013 Black Pawn</li> <li><code>bQ</code> \u2013 Black Queen</li> <li><code>bR</code> \u2013 Black Rook</li> </ul> <p>White Pieces (6):</p> <ul> <li><code>wB</code> \u2013 White Bishop</li> <li><code>wK</code> \u2013 White King</li> <li><code>wN</code> \u2013 White Knight</li> <li><code>wP</code> \u2013 White Pawn</li> <li><code>wQ</code> \u2013 White Queen</li> <li><code>wR</code> \u2013 White Rook</li> </ul> <p>Empty (1):</p> <ul> <li><code>xx</code> \u2013 Empty square</li> </ul>"},{"location":"architecture/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"architecture/#expected-results","title":"Expected Results","text":"<p>With the default configuration:</p> <ul> <li>Test Accuracy: ~99.90%</li> <li>F1 Score (Macro): ~99.90%</li> <li>Training Time: ~90 minutes (varies by hardware)</li> <li>Inference Speed: 0.05 ms per image (batch size 8192, varying by hardware)</li> </ul>"},{"location":"architecture/#per-class-performance","title":"Per-Class Performance","text":"<p>Actual accuracy by piece type (Test Dataset):</p> Class Accuracy Class Accuracy bB 99.90% wB 99.90% bK 100.00% wK 99.90% bN 100.00% wN 99.90% bP 99.81% wP 99.81% bQ 99.90% wQ 99.81% bR 100.00% wR 99.81% xx 100.00%"},{"location":"architecture/#evaluation-on-external-datasets","title":"Evaluation on External Datasets","text":"<p>The model has been evaluated on external datasets to assess generalization:</p>"},{"location":"architecture/#openboard","title":"OpenBoard","text":"<ul> <li>Dataset: S1M0N38/chess-cv-openboard</li> <li>Number of samples: 6,016</li> <li>Overall Accuracy: 99.30%</li> <li>F1 Score (Macro): 98.56%</li> </ul> <p>Per-class performance on OpenBoard:</p> Class Accuracy Class Accuracy bB 99.11% wB 100.00% bK 100.00% wK 100.00% bN 100.00% wN 98.97% bP 99.81% wP 99.61% bQ 97.10% wQ 98.48% bR 99.32% wR 98.03% xx 99.24%"},{"location":"architecture/#chessvision","title":"ChessVision","text":"<ul> <li>Dataset: S1M0N38/chess-cv-chessvision</li> <li>Number of samples: 3,186</li> <li>Overall Accuracy: 93.13%</li> <li>F1 Score (Macro): 92.28%</li> </ul> <p>Per-class performance on ChessVision:</p> Class Accuracy Class Accuracy bB 100.00% wB 95.87% bK 92.62% wK 99.09% bN 100.00% wN 99.09% bP 90.92% wP 92.26% bQ 98.92% wQ 85.06% bR 98.92% wR 96.69% xx 89.17% <p>Multi-Split Dataset</p> <p>The ChessVision dataset contains multiple splits. All splits are concatenated during evaluation to produce a single comprehensive score.</p> <p>Out of Sample Performance</p> <p>The lower performance on OpenBoard (99.30% accuracy, 98.56% F1) and ChessVision (93.13% accuracy, 92.28% F1) compared to the test set (99.90% accuracy, 99.90% F1) indicates some domain gap between the synthetic training data and these external datasets. ChessVision shows significantly lower performance, particularly on specific piece types like white queens (85.06%) and empty squares (89.17%).</p>"},{"location":"architecture/#dataset-characteristics","title":"Dataset Characteristics","text":""},{"location":"architecture/#synthetic-data-generation","title":"Synthetic Data Generation","text":"<p>The training data is synthetically generated:</p> <p>Source Materials:</p> <ul> <li>55 board styles (256\u00d7256px)</li> <li>64 piece sets (32\u00d732px)</li> <li>Multiple visual styles from chess.com and lichess</li> </ul> <p>Generation Process:</p> <ol> <li>Render each piece onto each board style</li> <li>Extract 32\u00d732 squares at piece locations</li> <li>Extract empty squares from light and dark squares</li> <li>Split combinations across train/val/test sets</li> </ol> <p>Data Statistics:</p> <ul> <li>Total Combinations: ~3,520 (55 boards \u00d7 64 piece sets)</li> <li>Images per Combination: 26 (12 pieces \u00d7 2 colors + 2 empty)</li> <li>Total Images: ~91,500</li> <li>Train Set: ~64,000 (70%)</li> <li>Validation Set: ~13,500 (15%)</li> <li>Test Set: ~13,500 (15%)</li> </ul>"},{"location":"architecture/#class-balance","title":"Class Balance","text":"<p>The dataset is perfectly balanced:</p> <ul> <li>Each class has equal representation</li> <li>Each board-piece combination contributes equally</li> <li>Train/val/test splits maintain class balance</li> </ul>"},{"location":"architecture/#arrows-model","title":"Arrows Model","text":""},{"location":"architecture/#model-architecture_1","title":"Model Architecture","text":""},{"location":"architecture/#overview","title":"Overview","text":"<p>The arrows model uses the same SimpleCNN architecture as the pieces model, but is trained to classify arrow overlay components instead of chess pieces. This enables detection and reconstruction of arrow annotations commonly used in chess analysis interfaces.</p>"},{"location":"architecture/#network-design_1","title":"Network Design","text":"<p>The network architecture is identical to the pieces model (see Pieces Model Architecture above), with the only difference being the output layer dimension.</p> <pre><code>[Same architecture as pieces model]\n\nFully Connected 2:\n\u2514\u2500\u2500 Linear(128 \u2192 49) \u2192 Output logits\n\nSoftmax \u2192 49-class probabilities\n</code></pre>"},{"location":"architecture/#model-statistics_1","title":"Model Statistics","text":"<ul> <li>Total Parameters: 156,077 (same as pieces model)</li> <li>Trainable Parameters: 156,077</li> <li>Model Size: ~645 KB (safetensors format)</li> <li>Input Size: 32\u00d732\u00d73 (RGB)</li> <li>Output Classes: 49</li> </ul>"},{"location":"architecture/#class-labels_1","title":"Class Labels","text":"<p>The model classifies chess squares into 49 categories representing arrow components:</p> <p>Arrow Heads (20):</p> <p>Directional arrow tips in 8 cardinal/ordinal directions plus intermediate angles:</p> <ul> <li><code>head-N</code>, <code>head-NNE</code>, <code>head-NE</code>, <code>head-ENE</code>, <code>head-E</code>, <code>head-ESE</code>, <code>head-SE</code>, <code>head-SSE</code></li> <li><code>head-S</code>, <code>head-SSW</code>, <code>head-SW</code>, <code>head-WSW</code>, <code>head-W</code>, <code>head-WNW</code>, <code>head-NW</code>, <code>head-NNW</code></li> </ul> <p>Arrow Tails (12):</p> <p>Directional arrow tails in 8 cardinal/ordinal directions plus intermediate angles:</p> <ul> <li><code>tail-N</code>, <code>tail-NNE</code>, <code>tail-NE</code>, <code>tail-ENE</code>, <code>tail-E</code>, <code>tail-ESE</code>, <code>tail-SE</code>, <code>tail-SSE</code></li> <li><code>tail-S</code>, <code>tail-SSW</code>, <code>tail-SW</code>, <code>tail-W</code></li> </ul> <p>Middle Segments (8):</p> <p>Arrow shaft segments for straight and diagonal lines:</p> <ul> <li><code>middle-N-S</code>, <code>middle-E-W</code>, <code>middle-NE-SW</code>, <code>middle-SE-NW</code></li> <li><code>middle-N-ENE</code>, <code>middle-E-SSE</code>, <code>middle-S-WSW</code>, <code>middle-W-NNW</code></li> <li><code>middle-N-WNW</code>, <code>middle-E-NNE</code>, <code>middle-S-ESE</code>, <code>middle-W-SSW</code></li> </ul> <p>Corners (4):</p> <p>Corner pieces for knight-move arrows (L-shaped patterns):</p> <ul> <li><code>corner-N-E</code>, <code>corner-E-S</code>, <code>corner-S-W</code>, <code>corner-W-N</code></li> </ul> <p>Empty (1):</p> <ul> <li><code>xx</code> \u2013 Empty square (no arrow)</li> </ul> <p>Naming Convention: NSEW refers to compass directions (North/South/East/West), indicating arrow orientation on the board from white's perspective.</p>"},{"location":"architecture/#performance-characteristics_1","title":"Performance Characteristics","text":""},{"location":"architecture/#expected-results_1","title":"Expected Results","text":"<p>With the default configuration:</p> <ul> <li>Test Accuracy: ~99.99%</li> <li>F1 Score (Macro): ~99.99%</li> <li>Training Time: ~9 minutes for 20 epochs (varies by hardware)</li> <li>Inference Speed: ~0.019 ms per image (batch size 512, varies by hardware)</li> </ul>"},{"location":"architecture/#per-class-performance_1","title":"Per-Class Performance","text":"<p>The arrows model achieves near-perfect accuracy across all 49 classes on the synthetic test dataset:</p> <p>Summary Statistics:</p> <ul> <li>Highest Accuracy: 100.00% (26 classes)</li> <li>Lowest Accuracy: 99.27% (tail-S)</li> <li>Mean Accuracy: 99.97%</li> <li>Classes &gt; 99.9%: 40 out of 49</li> </ul> <p>Performance by Component Type:</p> Component Type Classes Avg Accuracy Range Arrow Heads 20 99.97% 99.56% - 100% Arrow Tails 12 99.89% 99.27% - 100% Middle Segments 8 99.96% 99.78% - 100% Corners 4 99.98% 99.93% - 100% Empty Square 1 99.93% - <p>No External Dataset Evaluation</p> <p>Unlike the pieces model, the arrows model has only been evaluated on synthetic test data. No external datasets with annotated arrow components are currently available for out-of-distribution testing.</p>"},{"location":"architecture/#training-configuration","title":"Training Configuration","text":"<p>The arrows model uses different hyperparameters than the pieces model, optimized for the 49-class arrow classification task:</p> <ul> <li>Epochs: 20 (vs 200 for pieces - converges much faster)</li> <li>Batch Size: 128 (vs 64 for pieces - larger batches for more stable training)</li> <li>Learning Rate: 0.0005 (vs 0.0003 for pieces)</li> <li>Weight Decay: 0.00005 (vs 0.0003 for pieces - less regularization needed)</li> <li>Optimizer: AdamW</li> <li>Early Stopping: Disabled</li> </ul>"},{"location":"architecture/#dataset-characteristics_1","title":"Dataset Characteristics","text":""},{"location":"architecture/#synthetic-data-generation_1","title":"Synthetic Data Generation","text":"<p>The arrows training data is synthetically generated using the same board styles as the pieces model:</p> <p>Source Materials:</p> <ul> <li>55 board styles (256\u00d7256px)</li> <li>Arrow overlay images organized by component type</li> <li>Multiple visual styles from chess.com and lichess</li> </ul> <p>Generation Process:</p> <ol> <li>Render arrow components onto board backgrounds</li> <li>Extract 32\u00d732 squares at arrow locations</li> <li>Extract empty squares from light and dark squares</li> <li>Split combinations across train/val/test sets</li> </ol> <p>Data Statistics:</p> <ul> <li>Total Images: ~4.5 million</li> <li>Train Set: ~3,139,633 (70%)</li> <li>Validation Set: ~672,253 (15%)</li> <li>Test Set: ~672,594 (15%)</li> </ul> <p>The significantly larger dataset compared to pieces (~4.5M vs ~91K) is due to the combination of 55 boards \u00d7 49 arrow component types, with multiple arrow variants per component type.</p>"},{"location":"architecture/#class-balance_1","title":"Class Balance","text":"<p>The dataset maintains balanced class distribution:</p> <ul> <li>Each arrow component class has equal representation</li> <li>Each board-arrow combination contributes equally</li> <li>Train/val/test splits maintain class balance</li> </ul>"},{"location":"architecture/#limitations","title":"Limitations","text":"<p>Single Arrow Component Per Square</p> <p>The model is trained on images containing at most one arrow component per square. Classification accuracy degrades significantly when multiple arrow parts overlap in a single square, which can occur with densely annotated boards or crossing arrows.</p> <p>Example failure case: If a square contains both an arrow head and a perpendicular arrow shaft, the model may only detect one component or produce incorrect predictions.</p>"},{"location":"architecture/#snap-model","title":"Snap Model","text":""},{"location":"architecture/#model-architecture_2","title":"Model Architecture","text":""},{"location":"architecture/#overview_1","title":"Overview","text":"<p>The snap model uses the same SimpleCNN architecture as the pieces and arrows models, but is trained to classify piece centering quality instead of piece identity or arrow components. This enables automated detection of whether chess pieces are properly positioned within board squares, facilitating quality control for digital chess interfaces and automated analysis systems.</p>"},{"location":"architecture/#network-design_2","title":"Network Design","text":"<p>The network architecture is identical to the pieces model (see Pieces Model Architecture above), with the only difference being the output layer dimension.</p> <pre><code>[Same architecture as pieces model]\n\nFully Connected 2:\n\u2514\u2500\u2500 Linear(128 \u2192 2) \u2192 Output logits\n\nSoftmax \u2192 2-class probabilities\n</code></pre>"},{"location":"architecture/#model-statistics_2","title":"Model Statistics","text":"<ul> <li>Total Parameters: 156,077 (same as pieces model)</li> <li>Trainable Parameters: 156,077</li> <li>Model Size: ~600 KB (safetensors format)</li> <li>Input Size: 32\u00d732\u00d73 (RGB)</li> <li>Output Classes: 2</li> </ul>"},{"location":"architecture/#class-labels_2","title":"Class Labels","text":"<p>The model classifies chess squares into 2 categories representing piece centering quality:</p> <p>Centered (1):</p> <ul> <li><code>ok</code> \u2013 Pieces that are properly centered or slightly off-centered, plus empty squares</li> </ul> <p>Off-Centered (1):</p> <ul> <li><code>bad</code> \u2013 Pieces that are significantly misaligned or positioned poorly within the square</li> </ul> <p>Rationale: The model treats both properly centered pieces and empty squares as \"ok\" since both represent valid board states. Only poorly positioned pieces trigger the \"bad\" classification, enabling automated quality assurance.</p>"},{"location":"architecture/#performance-characteristics_2","title":"Performance Characteristics","text":""},{"location":"architecture/#expected-results_2","title":"Expected Results","text":"<p>With the default configuration:</p> <ul> <li>Test Accuracy: ~99.93%</li> <li>F1 Score (Macro): ~99.93%</li> <li>Training Time: TBD (training in progress, 200 epochs)</li> <li>Inference Speed: ~0.05 ms per image (similar to pieces model, varying by hardware)</li> </ul>"},{"location":"architecture/#per-class-performance_2","title":"Per-Class Performance","text":"<p>The snap model achieves excellent accuracy across both classes on the synthetic test dataset:</p> <p>Summary Statistics:</p> <ul> <li>Highest Accuracy: 99.98% (ok)</li> <li>Lowest Accuracy: 99.88% (bad)</li> <li>Mean Accuracy: 99.93%</li> <li>Classes &gt; 99.9%: 1 out of 2</li> </ul>"},{"location":"architecture/#evaluation-on-external-datasets_1","title":"Evaluation on External Datasets","text":"<p>No external dataset evaluation has been conducted yet. The model has only been evaluated on synthetic test data.</p>"},{"location":"architecture/#training-configuration_1","title":"Training Configuration","text":"<p>The snap model uses similar hyperparameters to the pieces model, optimized for the 2-class centering classification task:</p> <ul> <li>Epochs: 200 (same as pieces model)</li> <li>Batch Size: 64 (same as pieces model)</li> <li>Learning Rate: 0.001 with warmup and cosine decay (same as pieces model)</li> <li>Weight Decay: 0.001 (same as pieces model)</li> <li>Optimizer: AdamW</li> <li>Early Stopping: Disabled</li> </ul>"},{"location":"architecture/#dataset-characteristics_2","title":"Dataset Characteristics","text":""},{"location":"architecture/#synthetic-data-generation_2","title":"Synthetic Data Generation","text":"<p>The snap training data is synthetically generated using the same board styles as the pieces model:</p> <p>Source Materials:</p> <ul> <li>55 board styles (256\u00d7256px)</li> <li>64 piece sets (32\u00d732px)</li> <li>Multiple visual styles from chess.com and lichess</li> <li>Centered and off-centered piece positions</li> </ul> <p>Generation Process:</p> <ol> <li>Render pieces with intentional positioning variations</li> <li>Extract 32\u00d732 squares at piece locations</li> <li>Extract empty squares from light and dark squares</li> <li>Split combinations across train/val/test sets</li> </ol> <p>Data Statistics:</p> <ul> <li>Total Images: ~1.4M synthetic images</li> <li>Train Set: ~980,000 (70%)</li> <li>Validation Set: ~210,000 (15%)</li> <li>Test Set: ~210,000 (15%)</li> </ul> <p>The dataset is generated with 8 positional variations per piece-board combination:</p> <ul> <li>Non-empty pieces: 4 \"ok\" (centered/slightly off-centered) + 4 \"bad\" (significantly off-centered) variations</li> <li>Empty squares: 4 \"ok\" variations only (empty squares are always considered valid)</li> <li>This comprehensive variation strategy ensures robust centering detection across different board styles, piece sets, and positioning variations</li> </ul>"},{"location":"architecture/#class-balance_2","title":"Class Balance","text":"<p>The dataset maintains balanced class distribution:</p> <ul> <li>Each centering class has equal representation</li> <li>Empty squares are included in the \"ok\" class</li> <li>Train/val/test splits maintain class balance</li> </ul>"},{"location":"architecture/#limitations_1","title":"Limitations","text":"<p>Centering Semantics Preservation</p> <p>The model is trained with conservative augmentation to preserve centering semantics. No rotation or significant geometric transformations are applied that could alter the perceived centering of pieces within squares.</p> <p>Synthetic Training Data</p> <p>The model is trained only on synthetically generated centering variations. Performance on real-world chess board images with natural positioning variations may vary from synthetic test results.</p>"},{"location":"data-augmentations/","title":"Data Augmentation","text":"<p>Documentation of data augmentation strategies used in training each model.</p>"},{"location":"data-augmentations/#configuration","title":"Configuration","text":"<p>Augmentation parameters are defined in <code>src/chess_cv/constants.py</code>:</p> <pre><code>AUGMENTATION_CONFIGS = {\n    \"pieces\": {\n        \"padding\": 16,\n        \"padding_mode\": \"edge\",\n        \"rotation_degrees\": 10,\n        \"center_crop_size\": 40,\n        \"final_size\": 32,\n        \"resized_crop_scale\": (0.54, 0.74),\n        \"resized_crop_ratio\": (0.9, 1.1),\n        \"arrow_probability\": 0.80,\n        \"highlight_probability\": 0.25,\n        \"move_probability\": 0.50,\n        \"mouse_probability\": 0.90,\n        \"horizontal_flip\": True,\n        \"horizontal_flip_prob\": 0.5,\n        \"brightness\": 0.15,\n        \"contrast\": 0.2,\n        \"saturation\": 0.2,\n        \"hue\": 0.2,\n        \"noise_mean\": 0.0,\n        \"noise_sigma\": 0.05,\n    },\n    \"arrows\": {\n        \"arrow_probability\": 0.0,\n        \"highlight_probability\": 0.25,\n        \"move_probability\": 0.50,\n        \"scale_min\": 0.75,\n        \"scale_max\": 1.0,\n        \"horizontal_flip\": False,\n        \"brightness\": 0.20,\n        \"contrast\": 0.20,\n        \"saturation\": 0.20,\n        \"hue\": 0.2,\n        \"rotation_degrees\": 2,\n        \"noise_mean\": 0.0,\n        \"noise_sigma\": 0.10,\n    },\n    \"snap\": {\n        \"arrow_probability\": 0.50,\n        \"highlight_probability\": 0.20,\n        \"move_probability\": 0.50,\n        \"mouse_probability\": 0.80,\n        \"mouse_padding\": 134,\n        \"mouse_rotation_degrees\": 5,\n        \"mouse_center_crop_size\": 246,\n        \"mouse_final_size\": 32,\n        \"mouse_scale_range\": (0.20, 0.30),\n        \"mouse_ratio_range\": (0.8, 1.2),\n        \"horizontal_flip\": True,\n        \"horizontal_flip_prob\": 0.5,\n        \"brightness\": 0.15,\n        \"contrast\": 0.2,\n        \"saturation\": 0.2,\n        \"hue\": 0.2,\n        \"noise_mean\": 0.0,\n        \"noise_sigma\": 0.05,\n    },\n}\n</code></pre>"},{"location":"data-augmentations/#pieces-model-pipeline","title":"Pieces Model Pipeline","text":"<p>The first row shows original training images, while the second row displays their augmented versions for the pieces model.</p> <p>Applied in order during training:</p> <ol> <li> <p>Expand Canvas (16px padding): Pads image by 16px on all sides using edge replication mode (32\u00d732 \u2192 64\u00d764). Creates space for rotation without cropping piece edges.</p> </li> <li> <p>Random Rotation (\u00b110\u00b0): Rotates image with black fill. More aggressive than previous \u00b15\u00b0 to improve robustness.</p> </li> <li> <p>Center Crop (40\u00d740): Removes black corners introduced by rotation using conservative formula: <code>64 - (ceil(tan(10\u00b0) \u00d7 64) \u00d7 2) = 40</code>. Ensures no rotation artifacts remain.</p> </li> <li> <p>Random Resized Crop (area scale 0.54-0.74, aspect ratio 0.9-1.1, output 32\u00d732): Crops random region then resizes to 32\u00d732. Base area ratio (32/40)\u00b2 = 0.64 provides translation without zoom. Range 0.54-0.74 adds \u00b116% zoom variation. Aspect ratio 0.9-1.1 allows \u00b110% stretch for additional robustness.</p> </li> <li> <p>Arrow Overlay (80% probability): Overlays random arrow component from <code>data/arrows/</code>. Applied after geometric transforms to maintain crisp arrow graphics.</p> </li> <li> <p>Highlight Overlay (25% probability): Overlays semi-transparent highlight from <code>data/highlights/</code>.</p> </li> <li> <p>Move Overlay (50% probability): Overlays random move indicator (dot/ring) from <code>data/moves/</code>. Simulates move annotations on pieces during gameplay.</p> </li> <li> <p>Mouse Overlay (90% probability): Overlays random mouse cursor from <code>data/mouse/</code> with geometric transformations. Applies padding (134px), small rotation (\u00b15\u00b0), center crop (246\u00d7246), random resized crop to final size (32\u00d732) with scale 0.20-0.30 and ratio 0.8-1.2, making cursor smaller and positioning it randomly on the piece.</p> </li> <li> <p>Horizontal Flip (50% probability): Flips image left-to-right.</p> </li> <li> <p>Color Jitter: Randomly adjusts brightness (\u00b115%), contrast (\u00b120%), saturation (\u00b120%), and hue (\u00b120%).</p> </li> <li> <p>Gaussian Noise (\u03c3=0.05): Adds noise to normalized [0,1] pixels.</p> </li> </ol>"},{"location":"data-augmentations/#arrows-model-pipeline","title":"Arrows Model Pipeline","text":"<p>The first row shows original training images, while the second row displays their augmented versions for the arrows model.</p> <p>Applied in order during training:</p> <ol> <li> <p>Highlight Overlay (25% probability): Overlays semi-transparent highlight from <code>data/highlights/</code>. Applied early before other transforms.</p> </li> <li> <p>Move Overlay (50% probability): Overlays random move indicator (dot/ring) from <code>data/moves/</code>. Simulates move annotations on arrow components.</p> </li> <li> <p>Color Jitter: Randomly adjusts brightness, contrast, saturation by \u00b120%, and hue by \u00b120%.</p> </li> <li> <p>Random Rotation (\u00b12\u00b0): Small rotation to preserve arrow directionality.</p> </li> <li> <p>Gaussian Noise (\u03c3=0.10): Adds noise to normalized [0,1] pixels. Higher noise than pieces model.</p> </li> </ol>"},{"location":"data-augmentations/#snap-model-pipeline","title":"Snap Model Pipeline","text":"<p>The first row shows original training images with varying piece centering, while the second row displays their augmented versions for the snap model.</p> <p>Applied during preprocessing:</p> <ol> <li> <p>Piece Positioning: Applies translation to simulate different degrees of misalignment:</p> <ul> <li>\"ok\" class: 0-2px shifts (minimal/slight misalignment)</li> <li>\"bad\" class: 3-14px shifts (significant misalignment)</li> </ul> </li> <li> <p>Zoom Variation: Applies (-10%, +15%) random scaling to both classes for zoom robustness. Scaling is applied around the image center to preserve centering semantics while making the model resistant to different zoom levels.</p> </li> </ol> <p>Applied in order during training:</p> <ol> <li> <p>Arrow Overlay (50% probability): Overlays random arrow component from <code>data/arrows/</code>. Applied early to simulate realistic interface conditions where arrows may be present during piece positioning.</p> </li> <li> <p>Highlight Overlay (20% probability): Overlays semi-transparent highlight from <code>data/highlights/</code>. Simulates square highlighting that may occur during piece placement.</p> </li> <li> <p>Move Overlay (50% probability): Overlays random move indicator (dot/ring) from <code>data/moves/</code>. Simulates move indicators during piece positioning evaluation.</p> </li> <li> <p>Mouse Cursor Overlay (80% probability): Overlays random mouse cursor from <code>data/mouse/</code> with geometric transformations. Applies padding (134px), small rotation (\u00b15\u00b0), center crop (246\u00d7246), random resized crop to final size (32\u00d732) with scale 0.20-0.30 and ratio 0.8-1.2, making cursor smaller and positioning it randomly on the piece.</p> </li> <li> <p>Horizontal Flip (50% probability): Flips image left-to-right. Centering semantics are preserved under horizontal flip.</p> </li> <li> <p>Color Jitter: Randomly adjusts brightness (\u00b115%), contrast (\u00b120%), saturation (\u00b120%), and hue (\u00b120%).</p> </li> </ol> <p>Note: The snap model uses conservative augmentation during training with no additional geometric transformations like rotation, cropping, or further scaling beyond the zoom variation in preprocessing. This preserves piece centering semantics\u2014the model needs to distinguish between properly centered and poorly positioned pieces. Zoom variation in preprocessing (-10%, +15%) provides robustness to different zoom levels while maintaining the fundamental centering distinction.</p>"},{"location":"data-augmentations/#key-differences","title":"Key Differences","text":"Augmentation Pieces Arrows Snap Reason Canvas Expansion 16px edge padding (32\u219264) \u274c \u274c Creates rotation space without edge cropping Rotation \u00b110\u00b0 \u00b12\u00b0 \u274c Snap needs to preserve centering semantics Center Crop 40\u00d740 (removes black corners) \u274c \u274c Removes rotation artifacts Random Resized Crop Area 0.54-0.74, ratio 0.9-1.1 \u274c \u274c Translation + zoom would alter centering Arrow Overlay 80% \u274c 50% Simulates interface arrows during positioning Highlight Overlay 25% 25% 20% Simulates square highlighting Move Overlay 50% 50% 50% Simulates move indicators on pieces/arrows Mouse Overlay 90% \u274c 80% Simulates cursor interaction during placement Horizontal Flip 50% \u274c 50% Centering semantics preserved under flip Color Jitter B\u00b115%, CSH\u00b120% \u00b120% (BCSH) B\u00b115%, CSH\u00b120% Snap uses same variation as pieces Gaussian Noise \u03c3=0.05 \u03c3=0.10 \u03c3=0.05 Snap uses same noise level as pieces"},{"location":"data-augmentations/#implementation","title":"Implementation","text":"<p>The training script (<code>src/chess_cv/train.py</code>) constructs the pipeline dynamically based on model type:</p> <p>Pieces Model:</p> <pre><code># 1. Expand canvas (32\u00d732 \u2192 64\u00d764)\nv2.Pad(padding=16, padding_mode=\"edge\")\n\n# 2. Rotate with black fill\nv2.RandomRotation(degrees=10, fill=0)\n\n# 3. Remove black rotation artifacts (64\u00d764 \u2192 40\u00d740)\n# Formula: 64 - (ceil(tan(10\u00b0) \u00d7 64) \u00d7 2) = 40\nv2.CenterCrop(size=40)\n\n# 4. Random crop + zoom + resize (40\u00d740 \u2192 32\u00d732)\n# Area scale: (32/40)\u00b2 \u00b1 0.1 = 0.64 \u00b1 0.1 \u2192 (0.54, 0.74)\n# Aspect ratio: \u00b110% stretch \u2192 (0.9, 1.1)\nv2.RandomResizedCrop(size=32, scale=(0.54, 0.74), ratio=(0.9, 1.1))\n\n# 5-8. Overlays\nRandomArrowOverlay(probability=0.80)\nRandomHighlightOverlay(probability=0.25)\nRandomMoveOverlay(probability=0.50)\nRandomMouseOverlay(probability=0.90)\n\n# 9-10. Geometric + color\nv2.RandomHorizontalFlip(p=0.5)\nv2.ColorJitter(brightness=0.15, contrast=0.2, saturation=0.2, hue=0.2)\n\n# 11. Noise (requires tensor conversion)\nv2.ToImage() \u2192 v2.ToDtype() \u2192 v2.GaussianNoise() \u2192 v2.ToPILImage()\n</code></pre> <p>Arrows Model:</p> <pre><code># 1. Highlight overlay\nRandomHighlightOverlay(probability=0.25)\n\n# 2. Move overlay\nRandomMoveOverlay(probability=0.50)\n\n# 3-4. Color + rotation\nv2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\nv2.RandomRotation(degrees=2)\n\n# 5. Noise (requires tensor conversion)\nv2.ToImage() \u2192 v2.ToDtype() \u2192 v2.GaussianNoise() \u2192 v2.ToPILImage()\n</code></pre> <p>Snap Model:</p> <pre><code># 1-4. Overlays\nRandomArrowOverlay(probability=0.50)\nRandomHighlightOverlay(probability=0.20)\nRandomMoveOverlay(probability=0.50)\nRandomMouseOverlay(probability=0.80)\n\n# 5-6. Geometric + color\nv2.RandomHorizontalFlip(p=0.5)\nv2.ColorJitter(brightness=0.15, contrast=0.2, saturation=0.2, hue=0.2)\n\n# 7. Noise (requires tensor conversion)\nv2.ToImage() \u2192 v2.ToDtype() \u2192 v2.GaussianNoise() \u2192 v2.ToPILImage()\n</code></pre>"},{"location":"inference/","title":"Inference","text":"<p>Learn how to use Chess CV as a library and load pre-trained models.</p>"},{"location":"inference/#using-pre-trained-models","title":"Using Pre-trained Models","text":""},{"location":"inference/#using-bundled-models-recommended","title":"Using Bundled Models (Recommended)","text":"<p>The chess-cv package includes pre-trained weights for all three models. This is the simplest way to get started:</p> <pre><code>from chess_cv import load_bundled_model\n\n# Load models with bundled weights (included in package)\npieces_model = load_bundled_model('pieces')\narrows_model = load_bundled_model('arrows')\nsnap_model = load_bundled_model('snap')\n\n# Set to evaluation mode\npieces_model.eval()\narrows_model.eval()\nsnap_model.eval()\n\nprint(\"Models loaded successfully!\")\n</code></pre> <p>Get model configuration:</p> <pre><code>from chess_cv.constants import get_model_config\n\n# Get class names and other config for each model\npieces_config = get_model_config('pieces')\nprint(f\"Pieces classes: {pieces_config['class_names']}\")  # ['bB', 'bK', ..., 'xx']\nprint(f\"Number of classes: {pieces_config['num_classes']}\")  # 13\n\narrows_config = get_model_config('arrows')\nprint(f\"Number of arrow classes: {arrows_config['num_classes']}\")  # 49\n\nsnap_config = get_model_config('snap')\nprint(f\"Snap classes: {snap_config['class_names']}\")  # ['ok', 'bad']\nprint(f\"Number of classes: {snap_config['num_classes']}\")  # 2\n</code></pre> <p>Advanced: Get bundled weight paths:</p> <pre><code>from chess_cv import get_bundled_weight_path\nfrom chess_cv.model import SimpleCNN\nimport mlx.core as mx\n\n# Get path to bundled weights\nweight_path = get_bundled_weight_path('pieces')\nprint(f\"Bundled weights location: {weight_path}\")\n\n# Load manually if needed\nmodel = SimpleCNN(num_classes=13)\nweights = mx.load(str(weight_path))\nmodel.load_weights(list(weights.items()))\nmodel.eval()\n</code></pre>"},{"location":"inference/#loading-latest-version-from-hugging-face-hub","title":"Loading Latest Version from Hugging Face Hub","text":"<p>To get the latest version of the models (if updated after package release):</p> <pre><code>import mlx.core as mx\nfrom huggingface_hub import hf_hub_download\nfrom chess_cv.model import SimpleCNN\n\n# Download latest model weights from Hugging Face\nmodel_path = hf_hub_download(\n    repo_id=\"S1M0N38/chess-cv\",\n    filename=\"pieces.safetensors\"\n)\n\n# Create model and load weights\nmodel = SimpleCNN(num_classes=13)\nweights = mx.load(str(model_path))\nmodel.load_weights(list(weights.items()))\nmodel.eval()\n\nprint(\"Model loaded successfully!\")\n</code></pre>"},{"location":"inference/#making-predictions","title":"Making Predictions","text":""},{"location":"inference/#pieces-model-classify-chess-pieces","title":"Pieces Model - Classify Chess Pieces","text":"<p>Classify a chess square image to identify pieces:</p> <pre><code>import mlx.core as mx\nimport numpy as np\nfrom PIL import Image\nfrom chess_cv import load_bundled_model\nfrom chess_cv.constants import get_model_config\n\n# Load model\nmodel = load_bundled_model('pieces')\nmodel.eval()\n\n# Get class names\nclasses = get_model_config('pieces')['class_names']\n\n# Load and preprocess image\ndef preprocess_image(image_path: str) -&gt; mx.array:\n    \"\"\"Load and preprocess a chess square image.\n\n    Args:\n        image_path: Path to 32\u00d732 RGB image\n\n    Returns:\n        Preprocessed image tensor ready for model\n    \"\"\"\n    # Load image\n    img = Image.open(image_path).convert('RGB')\n    img = img.resize((32, 32))\n\n    # Convert to array and normalize\n    img_array = np.array(img, dtype=np.float32) / 255.0\n\n    # Add batch dimension and convert to MLX array\n    # MLX uses NHWC format: (batch, height, width, channels)\n    img_tensor = mx.array(img_array[None, ...])\n\n    return img_tensor\n\n# Make prediction\nimage_tensor = preprocess_image(\"square.png\")\nlogits = model(image_tensor)\nprobabilities = mx.softmax(logits, axis=-1)\npredicted_class = mx.argmax(probabilities, axis=-1).item()\n\nprint(f\"Predicted class: {classes[predicted_class]}\")\nprint(f\"Confidence: {probabilities[0, predicted_class].item():.2%}\")\n</code></pre>"},{"location":"inference/#arrows-model-classify-arrow-components","title":"Arrows Model - Classify Arrow Components","text":"<p>Classify arrow overlay components on chess board squares:</p> <pre><code>import mlx.core as mx\nimport numpy as np\nfrom PIL import Image\nfrom chess_cv import load_bundled_model\nfrom chess_cv.constants import get_model_config\n\n# Load arrows model\nmodel = load_bundled_model('arrows')\nmodel.eval()\n\n# Get class names (49 arrow components + empty)\nclasses = get_model_config('arrows')['class_names']\n\n# Preprocess and predict (using same preprocess_image function as above)\nimage_tensor = preprocess_image(\"square_with_arrow.png\")\nlogits = model(image_tensor)\nprobabilities = mx.softmax(logits, axis=-1)\npredicted_class = mx.argmax(probabilities, axis=-1).item()\n\nprint(f\"Predicted arrow component: {classes[predicted_class]}\")\nprint(f\"Confidence: {probabilities[0, predicted_class].item():.2%}\")\n</code></pre> <p>Arrow Classes: The arrows model classifies 49 components including arrow heads (e.g., <code>head-N</code>, <code>head-SE</code>), tails (e.g., <code>tail-W</code>), middle segments (e.g., <code>middle-N-S</code>), corners (e.g., <code>corner-N-E</code>), and empty squares (<code>xx</code>).</p>"},{"location":"inference/#snap-model-classify-piece-centering","title":"Snap Model - Classify Piece Centering","text":"<p>Detect whether chess pieces are properly centered in squares:</p> <pre><code>import mlx.core as mx\nimport numpy as np\nfrom PIL import Image\nfrom chess_cv import load_bundled_model\nfrom chess_cv.constants import get_model_config\n\n# Load snap model\nmodel = load_bundled_model('snap')\nmodel.eval()\n\n# Get class names ('ok' or 'bad')\nclasses = get_model_config('snap')['class_names']\n\n# Preprocess and predict (using same preprocess_image function as above)\nimage_tensor = preprocess_image(\"square_to_check.png\")\nlogits = model(image_tensor)\nprobabilities = mx.softmax(logits, axis=-1)\npredicted_class = mx.argmax(probabilities, axis=-1).item()\n\nprint(f\"Piece centering: {classes[predicted_class]}\")\nprint(f\"Confidence: {probabilities[0, predicted_class].item():.2%}\")\n\nif classes[predicted_class] == 'bad':\n    print(\"\u26a0\ufe0f Piece is off-centered - needs adjustment\")\nelse:\n    print(\"\u2713 Piece is properly centered or square is empty\")\n</code></pre> <p>Use Cases: The snap model is useful for automated board state validation, piece positioning quality control, and chess interface usability testing.</p>"},{"location":"inference/#batch-predictions","title":"Batch Predictions","text":"<p>Process multiple images efficiently:</p> <pre><code>import mlx.core as mx\nfrom pathlib import Path\nfrom chess_cv import load_bundled_model\nfrom chess_cv.constants import get_model_config\nfrom chess_cv.model import SimpleCNN\n\n# Load model and get class names\nmodel = load_bundled_model('pieces')\nmodel.eval()\nclasses = get_model_config('pieces')['class_names']\n\ndef predict_batch(model: SimpleCNN, image_paths: list[str], classes: list[str]) -&gt; list[dict]:\n    \"\"\"Predict classes for multiple images.\n\n    Args:\n        model: Trained SimpleCNN model\n        image_paths: List of paths to chess square images\n        classes: List of class names\n\n    Returns:\n        List of prediction dictionaries with class and confidence\n    \"\"\"\n    # Preprocess all images\n    images = [preprocess_image(path) for path in image_paths]\n    batch = mx.concatenate(images, axis=0)\n\n    # Make predictions\n    logits = model(batch)\n    probabilities = mx.softmax(logits, axis=-1)\n    predicted_classes = mx.argmax(probabilities, axis=-1)\n\n    # Format results\n    results = []\n    for i, path in enumerate(image_paths):\n        pred_idx = predicted_classes[i].item()\n        confidence = probabilities[i, pred_idx].item()\n        results.append({\n            'path': path,\n            'class': classes[pred_idx],\n            'confidence': confidence\n        })\n\n    return results\n\n# Example usage\nimage_paths = [\"square1.png\", \"square2.png\", \"square3.png\"]\npredictions = predict_batch(model, image_paths, classes)\n\nfor pred in predictions:\n    print(f\"{pred['path']}: {pred['class']} ({pred['confidence']:.2%})\")\n</code></pre>"},{"location":"inference/#troubleshooting","title":"Troubleshooting","text":"<p>Model Loading Issues: If model loading fails, verify the file path exists and that the model architecture matches the weights. Use <code>SimpleCNN(num_classes=13)</code> for pieces, <code>SimpleCNN(num_classes=49)</code> for arrows, or <code>SimpleCNN(num_classes=2)</code> for snap.</p> <p>Memory Issues: Process images in smaller batches if you encounter memory problems during batch prediction.</p> <p>Wrong Predictions: Ensure input images are properly preprocessed (32\u00d732px, RGB, normalized to [0,1]).</p>"},{"location":"inference/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the Architecture documentation for model details</li> <li>Check out Train and Evaluate for training custom models</li> </ul>"},{"location":"setup/","title":"Setup Guide","text":"<p>This guide will help you install and configure Chess CV for inference and training chess board classification models.</p>"},{"location":"setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.13+: Chess CV requires Python 3.13 or later</li> <li>uv: Fast Python package manager (required for training, installation guide)</li> <li>MLX: Apple's machine learning framework (installed with <code>chess-cv</code>)</li> </ul>"},{"location":"setup/#installation","title":"Installation","text":""},{"location":"setup/#for-inference-only","title":"For Inference Only","text":"<p>Install Chess CV directly from PyPI:</p> <pre><code>pip install chess-cv\n# or with uv\nuv add chess-cv\n</code></pre>"},{"location":"setup/#for-training-and-evaluation","title":"For Training and Evaluation","text":"<p>Clone the repository and install all dependencies:</p> <pre><code># Clone the repository\ngit clone https://github.com/S1M0N38/chess-cv.git\ncd chess-cv\n\n# Copy environment template\ncp .envrc.example .envrc\n\n# Install all dependencies\nuv sync --all-extras\n\n# For contributors: add development tools\nuv sync --all-extras --group dev\n</code></pre>"},{"location":"setup/#next-steps","title":"Next Steps","text":"<ul> <li>Use pre-trained models for inference with Model Usage</li> <li>Train custom models with Train and Evaluate</li> <li>Contribute to the project with CONTRIBUTING.md</li> </ul>"},{"location":"train-and-eval/","title":"Train and Evaluate","text":"<p>Learn how to generate data, train models, and evaluate performance with Chess CV.</p>"},{"location":"train-and-eval/#data-generation","title":"Data Generation","text":""},{"location":"train-and-eval/#basic-usage","title":"Basic Usage","text":"<p>Generate synthetic training data for a specific model:</p> <pre><code># Generate data for pieces model (default: 70% train, 15% val, 15% test)\nchess-cv preprocessing pieces\n\n# Generate data for arrows model\nchess-cv preprocessing arrows\n\n# Generate data for snap model\nchess-cv preprocessing snap\n</code></pre>"},{"location":"train-and-eval/#custom-output-directories","title":"Custom Output Directories","text":"<p>If you need to specify custom output directories:</p> <pre><code>chess-cv preprocessing pieces \\\n  --train-dir custom/pieces/train \\\n  --val-dir custom/pieces/validate \\\n  --test-dir custom/pieces/test\n</code></pre> <p>Note: The default 70/15/15 train/val/test split with seed 42 is used. These values are defined in <code>src/chess_cv/constants.py</code> and provide consistent, reproducible splits.</p>"},{"location":"train-and-eval/#understanding-data-generation","title":"Understanding Data Generation","text":"<p>The preprocessing script generates model-specific training data:</p> <p>Pieces Model:</p> <ol> <li>Reads board images from <code>data/boards/</code> and piece sets from <code>data/pieces/</code></li> <li>For each board-piece combination:<ul> <li>Renders pieces onto the board</li> <li>Extracts 32\u00d732 pixel squares</li> <li>Saves images to train/validate/test directories</li> </ul> </li> <li>Generates ~93,000 images split into train (70%), val (15%), test (15%)</li> <li>Balanced across 13 classes (12 pieces + empty square)</li> </ol> <p>Arrows Model:</p> <ol> <li>Reads board images and arrow overlays from <code>data/arrows/</code></li> <li>For each board-arrow combination:<ul> <li>Renders arrow components onto boards</li> <li>Extracts 32\u00d732 pixel squares</li> </ul> </li> <li>Generates ~4.5M images for 49 arrow component classes</li> </ol> <p>Snap Model:</p> <ol> <li>Reads board images and piece sets with positioning variations</li> <li>Generates centered and off-centered piece examples</li> <li>Generates ~1.4M synthetic images for 2 centering classes (ok/bad)</li> </ol> <p>All models use the 70/15/15 train/val/test split with seed 42 for reproducibility.</p>"},{"location":"train-and-eval/#model-training","title":"Model Training","text":""},{"location":"train-and-eval/#basic-training","title":"Basic Training","text":"<p>Train a specific model with default settings:</p> <pre><code># Train pieces model\nchess-cv train pieces\n\n# Train arrows model\nchess-cv train arrows\n\n# Train snap model\nchess-cv train snap\n</code></pre>"},{"location":"train-and-eval/#custom-training-configuration","title":"Custom Training Configuration","text":"<pre><code>chess-cv train pieces \\\n  --train-dir data/splits/pieces/train \\\n  --val-dir data/splits/pieces/validate \\\n  --checkpoint-dir checkpoints/pieces \\\n  --batch-size 64 \\\n  --weight-decay 0.001 \\\n  --num-epochs 1000 \\\n  --num-workers 8\n</code></pre> <p>Model-Specific Defaults:</p> <p>Each model type uses optimized hyperparameters defined in <code>src/chess_cv/constants.py</code>. The arrows model, for example, uses larger batch sizes (128) and fewer epochs (20) as it converges faster.</p> <p>Note: Image size is fixed at 32\u00d732 pixels (model architecture requirement).</p>"},{"location":"train-and-eval/#training-parameters","title":"Training Parameters","text":"<p>Optimizer Settings:</p> <ul> <li><code>--weight-decay</code>: Weight decay for regularization (default: 0.001)</li> </ul> <p>Learning Rate Scheduler (enabled by default):</p> <ul> <li>Base LR: 0.001 (peak after warmup)</li> <li>Min LR: 1e-5 (end of cosine decay)</li> <li>Warmup: 3% of total steps (~30 epochs for 1000-epoch training)</li> </ul> <p>Training Control:</p> <ul> <li><code>--num-epochs</code>: Maximum number of epochs (default: 200, recent models use 1000)</li> <li><code>--batch-size</code>: Batch size for training (default: 64)</li> </ul> <p>Data Settings:</p> <ul> <li><code>--num-workers</code>: Number of data loading workers (default: 8)</li> </ul> <p>Directories:</p> <ul> <li><code>--train-dir</code>: Training data directory (default: data/splits/pieces/train)</li> <li><code>--val-dir</code>: Validation data directory (default: data/splits/pieces/validate)</li> <li><code>--checkpoint-dir</code>: Where to save model checkpoints (default: checkpoints)</li> </ul>"},{"location":"train-and-eval/#training-features","title":"Training Features","text":"<p>Learning Rate Schedule:</p> <ul> <li>Warmup phase: linear increase from 0 to 0.001 over first 3% of steps</li> <li>Cosine decay: gradual decrease from 0.001 to 1e-5 over remaining steps</li> </ul> <p>Data Augmentation:</p> <ul> <li>Random resized crop (scale: 0.54-0.74)</li> <li>Random horizontal flip</li> <li>Color jitter (brightness: \u00b10.15, contrast/saturation/hue: \u00b10.2)</li> <li>Random rotation (\u00b110\u00b0)</li> <li>Gaussian noise (std: 0.05)</li> <li>Arrow overlay (80% probability)</li> <li>Highlight overlay (25% probability)</li> </ul> <p>Early Stopping:</p> <p>Early stopping is disabled by default (patience set to 999999), allowing the full training schedule to run. This default is set in <code>src/chess_cv/constants.py</code> and ensures consistent training across runs.</p> <p>Automatic Checkpointing:</p> <ul> <li>Best model weights saved to <code>checkpoints/{model-id}/{model-id}.safetensors</code></li> <li>Optimizer state saved to <code>checkpoints/optimizer.safetensors</code></li> </ul>"},{"location":"train-and-eval/#training-output","title":"Training Output","text":"<p>Files Generated:</p> <ul> <li><code>checkpoints/{model-id}/{model-id}.safetensors</code> \u2013 Best model weights</li> <li><code>checkpoints/optimizer.safetensors</code> \u2013 Optimizer state</li> <li><code>outputs/training_curves.png</code> \u2013 Loss and accuracy plots</li> <li><code>outputs/augmentation_example.png</code> \u2013 Example of data augmentation</li> </ul>"},{"location":"train-and-eval/#experiment-tracking","title":"Experiment Tracking","text":""},{"location":"train-and-eval/#weights-biases-integration","title":"Weights &amp; Biases Integration","text":"<p>Track experiments with the W&amp;B dashboard by adding the <code>--wandb</code> flag:</p> <pre><code># First time setup\nwandb login\n\n# Train with wandb logging\nchess-cv train pieces --wandb\n</code></pre> <p>Features: Real-time metric logging, hyperparameter tracking, model comparison, and experiment organization.</p>"},{"location":"train-and-eval/#hyperparameter-sweeps","title":"Hyperparameter Sweeps","text":"<p>Optimize hyperparameters with W&amp;B sweeps using the integrated <code>--sweep</code> flag:</p> <pre><code># First time setup\nwandb login\n\n# Run hyperparameter sweep for a model (requires --wandb)\nchess-cv train pieces --sweep --wandb\n\n# The sweep will use the configuration defined in src/chess_cv/sweep.py\n</code></pre> <p>Important: The <code>--sweep</code> flag requires <code>--wandb</code> to be enabled. The sweep configuration is defined in <code>src/chess_cv/sweep.py</code> and includes parameters like learning rate, batch size, and weight decay optimized for each model type.</p>"},{"location":"train-and-eval/#model-evaluation","title":"Model Evaluation","text":""},{"location":"train-and-eval/#basic-evaluation","title":"Basic Evaluation","text":"<p>Evaluate a trained model on its test set:</p> <pre><code># Evaluate pieces model\nchess-cv test pieces\n\n# Evaluate arrows model\nchess-cv test arrows\n\n# Evaluate snap model\nchess-cv test snap\n</code></pre>"},{"location":"train-and-eval/#custom-evaluation","title":"Custom Evaluation","text":"<pre><code>chess-cv test pieces \\\n  --test-dir data/splits/pieces/test \\\n  --train-dir data/splits/pieces/train \\\n  --checkpoint checkpoints/pieces/pieces.safetensors \\\n  --batch-size 64 \\\n  --num-workers 8 \\\n  --output-dir outputs/pieces\n</code></pre>"},{"location":"train-and-eval/#evaluating-on-external-datasets","title":"Evaluating on External Datasets","text":"<p>Test model performance on HuggingFace datasets:</p> <pre><code># Evaluate on a specific dataset\nchess-cv test pieces \\\n  --hf-test-dir S1M0N38/chess-cv-openboard\n\n# Concatenate all splits from the dataset\nchess-cv test pieces \\\n  --hf-test-dir S1M0N38/chess-cv-chessvision \\\n  --concat-splits\n</code></pre>"},{"location":"train-and-eval/#evaluation-output","title":"Evaluation Output","text":"<p>Files Generated:</p> <ul> <li><code>outputs/test_confusion_matrix.png</code> \u2013 Confusion matrix heatmap</li> <li><code>outputs/test_per_class_accuracy.png</code> \u2013 Per-class accuracy bar chart</li> <li><code>outputs/misclassified_images/</code> \u2013 Misclassified examples for analysis</li> </ul>"},{"location":"train-and-eval/#analyzing-results","title":"Analyzing Results","text":"<p>Confusion Matrix:</p> <p>Shows where the model makes mistakes. Look for:</p> <ul> <li>High off-diagonal values (common misclassifications)</li> <li>Patterns in similar piece types (e.g., knights vs bishops)</li> </ul> <p>Misclassified Images:</p> <p>Review examples in <code>outputs/misclassified_images/</code> to understand:</p> <ul> <li>Which board/piece combinations are challenging</li> <li>Whether augmentation needs adjustment</li> <li>If more training data would help</li> </ul>"},{"location":"train-and-eval/#model-deployment","title":"Model Deployment","text":""},{"location":"train-and-eval/#upload-to-hugging-face-hub","title":"Upload to Hugging Face Hub","text":"<p>Share your trained models on Hugging Face Hub:</p> <pre><code># First time setup\nhf login\n\n# Upload a specific model\nchess-cv upload pieces --repo-id username/chess-cv\n</code></pre> <p>Examples:</p> <pre><code># Upload pieces model with default settings\nchess-cv upload pieces --repo-id username/chess-cv\n\n# Upload arrows model with custom message\nchess-cv upload arrows --repo-id username/chess-cv \\\n  --message \"feat: improved arrows model v2\"\n\n# Upload to private repository\nchess-cv upload snap --repo-id username/chess-cv --private\n\n# Specify custom paths\nchess-cv upload pieces --repo-id username/chess-cv \\\n  --checkpoint-dir ./my-checkpoints/pieces \\\n  --readme docs/custom_README.md\n</code></pre> <p>What gets uploaded: Model weights (<code>{model-id}.safetensors</code>), model card with metadata, and model configuration.</p>"},{"location":"train-and-eval/#troubleshooting","title":"Troubleshooting","text":"<p>Out of Memory During Training: Reduce batch size with <code>--batch-size 64</code> or reduce number of workers with <code>--num-workers 2</code>.</p> <p>Poor Model Performance: Try adjusting hyperparameters with W&amp;B sweeps for optimization, or review misclassified images to verify data quality. To enable early stopping for faster experimentation, modify <code>DEFAULT_PATIENCE</code> in <code>src/chess_cv/constants.py</code>.</p> <p>Training Too Slow: Increase batch size if memory allows (<code>--batch-size 128</code>). For faster experimentation, modify <code>DEFAULT_PATIENCE</code> in <code>src/chess_cv/constants.py</code> to enable early stopping.</p> <p>Evaluation Issues: Ensure the checkpoint exists, verify the test data directory is populated, and run with appropriate batch size.</p>"},{"location":"train-and-eval/#next-steps","title":"Next Steps","text":"<ul> <li>Use your trained model for inference with Model Usage</li> <li>Explore model internals with Architecture</li> <li>Share your model on Hugging Face Hub using the upload command above</li> </ul>"}]}